#### 目标
为了测试LLM的联网RAG能力，我们需要制定相应的问题数据集。根据评测标准评价LLM的RAG性能。

#### 解析
我们制作的问题一定是只有在网络搜索的条件下才能得到对应问题的正确答案的。
5万个问题庞大，选择让现有的LLM生成。
所以问题转化为如何让LLM获取广泛的互联网信息，并进一步生成多样性、可靠性的问题。

#### 解决思路
1. 使用精心设计的prompt, 结合LLM的联网搜索功能，让他自己寻找topic, 并根据topic整合信息，生成问题。
    >弊端是LLM也不知道自己要找什么话题，从RAG的角度来讲，我们无论如何精心的设计prompt, 都不能命中LLM联网搜索的知识库，所以要给出LLM具体的话题，提高命中概率。

2. 如何有效收集具体的topic呢（比如关于“乌克兰签署矿产协议”）。NLP中这样的任务叫话题识别，是语义级别的任务，做起来麻烦（因为要联网，不是私有数据集）。所以我们干脆直接爬取微博、知乎热题，把他们做成topic。

3. 另一种有效收集topic方案是是使用LLM的联网功能，先行一步，收集网络话题（比如收集100个），然后将这些话题逐一做成prompt SQL,在让LLM生成问题，这样命中率会有极大的提升。比如我们可以这样问LLM：
    >帮我收集2024年12月份的网络热议话题比如从央视网、微博、知乎、虎扑、贴吧、抖音、sohu等平台收集。返回给我100个热议话题，不要给我多余的回答。

4. 我们将收集到的topic做成topics列表,下一步是根据列表中的热词，收集网络资料，逐一生成回答。通过遍历热词列表，我们可以实现问题生成自动化。经过各家厂商的测试，发现阶跃星辰的具有联网tool的API最好用。（一般厂商的API都遵循openai的格式规范，所以只需要替换API密钥和模型选项即可）

#### 问题 
- 搜索范围单一，总是从news.qq.com收集信息
~~太慢，生成时间和topic在网络上的出现频次（热度）有关。比如“GPT4.5”话题要比“杨紫换装”话题产生对应问题的速度更快。猜测速度瓶颈主要在搜索上~~
- 找到了太慢的原因是LLM回复内容的复杂度。比如让LLM返回 “问题-参考答案-来源” 远比 返回 “问题” 时间开销大。前者平均时长100s, 后者平均时长15s
- 模型参数对搜索性能有影响？？

#### cost计算（2个问题 <- 1个话题）
钱:  
21个话题->40个问题 （一个话题违法）  
10.34-0.91 = 0.93 r  
0.93 * 50000 / 24 = 1937.5 r  
存储：  
json格式：150KB / 40个问题（20个话题）  
txt纯问题格式：60KB / 40个问题（20个话题）  






